<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>python on Engineer Dancun</title>
    <link>https://devopsengineerdan.github.io/tags/python/</link>
    <description>Recent content in python on Engineer Dancun</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 09 Sep 2023 13:30:03 +0300</lastBuildDate>
    
	<atom:link href="https://devopsengineerdan.github.io/tags/python/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Best Practices for Engineering ML Pipelines - Part 2</title>
      <link>https://devopsengineerdan.github.io/posts/best-practices-for-engineering-data-pipelines-part2/</link>
      <pubDate>Sat, 09 Sep 2023 13:30:03 +0300</pubDate>
      
      <guid>https://devopsengineerdan.github.io/posts/best-practices-for-engineering-data-pipelines-part2/</guid>
      <description>This is the second part in a series of articles demonstrating best practices for engineering ML pipelines and deploying them to production. In the first part we focused on project setup - everything from codebase structure to configuring a CI/CD pipeline and making an initial deployment of a skeleton pipeline.
In this part we are going to focus on developing a fully-operational pipeline and will cover:
 A simple approach to data and model versioning, using cloud object storage.</description>
    </item>
    
    <item>
      <title>Best Practices for Engineering ML Pipelines - Part 1</title>
      <link>https://devopsengineerdan.github.io/posts/best-practices-for-engineering-data-pipelines-part1/</link>
      <pubDate>Sat, 09 Sep 2023 13:29:58 +0300</pubDate>
      
      <guid>https://devopsengineerdan.github.io/posts/best-practices-for-engineering-data-pipelines-part1/</guid>
      <description>The is the first in a series of articles demonstrating how to engineer a machine learning pipeline and deploy it to a production environment. Weâ€™re going to assume that a solution to a ML problem already exists within a Jupyter notebook, and that our task is to engineer this solution into an operational ML system, that can train a model, serve it via a web API and automatically repeat this process on a schedule when new data is made available.</description>
    </item>
    
    <item>
      <title>Deploying ML Models with Bodywork</title>
      <link>https://devopsengineerdan.github.io/posts/deploying-ml-models-with-bodywork/</link>
      <pubDate>Sat, 09 Sep 2023 13:28:50 +0300</pubDate>
      
      <guid>https://devopsengineerdan.github.io/posts/deploying-ml-models-with-bodywork/</guid>
      <description>Deploying ML Models with Bodywork Solutions to ML problems are usually first developed in Jupyter notebooks. We are then faced with an altogether different problem - how to engineer these notebook solutions into your products and systems and continue to maintain their performance through time, after new data is generated.
 Table of Contents   What is this Tutorial Going to Teach Me? Introduction  Why is MLOps Getting so Much Attention?</description>
    </item>
    
    <item>
      <title>Best Practices for PySpark ETL Projects</title>
      <link>https://devopsengineerdan.github.io/posts/best-practices-for-pyspark-etl/</link>
      <pubDate>Sat, 09 Sep 2023 13:27:53 +0300</pubDate>
      
      <guid>https://devopsengineerdan.github.io/posts/best-practices-for-pyspark-etl/</guid>
      <description>I have often lent heavily on Apache Spark and the SparkSQL APIs for operationalising any type of batch data-processing &amp;lsquo;job&amp;rsquo;, within a production environment where handling fluctuating volumes of data reliably and consistently are on-going business concerns. These batch data-processing jobs may involve nothing more than joining data sources and performing aggregations, or they may apply machine learning models to generate inventory recommendations - regardless of the complexity, this often reduces to defining Extract, Transform and Load (ETL) jobs.</description>
    </item>
    
    <item>
      <title> Deploying machine learning models on kubernetes</title>
      <link>https://devopsengineerdan.github.io/posts/deploying-ml-models-on-kubernetes/</link>
      <pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://devopsengineerdan.github.io/posts/deploying-ml-models-on-kubernetes/</guid>
      <description>Deploying Machine Learning Models on Kubernetes Bodywork A common pattern for deploying Machine Learning (ML) models into production environments - e.g. ML models trained using the SciKit Learn or Keras packages (for Python), that are ready to provide predictions on new data - is to expose these ML as RESTful API microservices, hosted from within Docker containers. These can then deployed to a cloud environment for handling everything required for maintaining continuous availability - e.</description>
    </item>
    
    <item>
      <title>Implementing py-package-template</title>
      <link>https://devopsengineerdan.github.io/posts/py-package-template/</link>
      <pubDate>Sun, 13 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://devopsengineerdan.github.io/posts/py-package-template/</guid>
      <description>Python Package Template Project  
The py-template-project package allows users to download the contents of this GiHub repository, containing a skeleton Python package project to be used as a template for kick-starting development of any type of Package; destined for upload to PyPI, or just for local install using Pip. The downloaded package includes the following components to aid rapid development without having to spend time cloning existing set-ups from other projects:</description>
    </item>
    
    <item>
      <title>Machine learning workflow automation</title>
      <link>https://devopsengineerdan.github.io/posts/ml-worlflow-automation/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://devopsengineerdan.github.io/posts/ml-worlflow-automation/</guid>
      <description>Automating the Archetypal Machine Learning Workflow and Model Deployment This repository contains a Python-based Machine Learning (ML) project, whose primary aim is to demonstrate the archetypal ML workflow within a Jupyter notebook, together with some proof-of-concept ideas on automating key steps, using the Titanic binary classification dataset hosted on Kaggle. The ML workflow includes: data exploration and visualisation, feature engineering, model training and selection. The notebook - titanic-ml.ipynb - also yields a persisted prediction pipeline (pickled to the models directory), that is used downstream in the model deployment process.</description>
    </item>
    
    <item>
      <title>Implementing machine learning bodywork</title>
      <link>https://devopsengineerdan.github.io/posts/ml-bodywork-core/</link>
      <pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://devopsengineerdan.github.io/posts/ml-bodywork-core/</guid>
      <description>Machine Learning Bodywork Core Bodywork deploys machine learning projects developed in Python, to Kubernetes. It helps you to:
 serve models as microservices execute batch jobs run reproducible pipelines  On demand, or on a schedule. It automates repetitive DevOps tasks and frees machine learning engineers to focus on what they do best - solving data problems with machine learning.
Get Started Bodywork is distributed as a Python package - install it from PyPI:</description>
    </item>
    
    <item>
      <title>MicroPython in action</title>
      <link>https://devopsengineerdan.github.io/posts/micropython-in-action/</link>
      <pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://devopsengineerdan.github.io/posts/micropython-in-action/</guid>
      <description>Watch MicroPython in action  
The MicroPython project This is the MicroPython project, which aims to put an implementation of Python 3.x on microcontrollers and small embedded systems. You can find the official website at micropython.org.
WARNING: this project is in beta stage and is subject to changes of the code-base, including project-wide name changes and API changes.
MicroPython implements the entire Python 3.4 syntax (including exceptions, with, yield from, etc.</description>
    </item>
    
  </channel>
</rss>